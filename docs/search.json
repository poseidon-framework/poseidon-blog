[
  {
    "objectID": "posts/trident-ordered-forge.html",
    "href": "posts/trident-ordered-forge.html",
    "title": "Trident update: Ordered forge",
    "section": "",
    "text": "Image of a shopping list\n\n\nDuring 2024, a lot of features were added to our flagship software trident. One of them was the --ordered flag to trident forge.\nAs you may know, trident forge is one of our most advanced and powerful subcommands. A forge merges packages and subselects from them at the same time, forming a custom-collection of genomes (ancient or modern) pulled and merged from a large selection of packages. As I’ve introduced in a lot more detail here, you can easily select a number of groups or individuals, for example via\ntrident forge -d /path/to/poseidon_packages -f \"French, Finnish, Spanish\"\nwhich will then go and select from the dozens or hundreds of packages those that contain these groups and output only those individuals from these three groups into a new custom-built Poseidon package. Now, in our original implementation, the order of samples would simply follow the order that the samples originally appeared in their respective packages. So if you have a setup like this:\n- Package1:\n  - Francois (French)\n  - Pedro (Spanish)\n  - Vladimir (Russian)\n  - Mika (Finnish)\n- Package2:\n  - Michele (French)\n  - Sanni (Finnish)\nthe output will conist of\n- Francois (French)\n- Pedro (Spanish)\n- Mika (Finnish)\n- Michele (French)\n- Sanni (Finnish)\nwhich means that your groups are mixed and not ordered. Often you don’t care about the order, but sometimes you do. In particular, since trident forge not just outputs genotype data, but also gives you a custom new Janno annotation file, which is a table of all meta-information for your samples. Having this ordered can be helpful.\nSo in Release 1.5.4.0, we introduced a new flag --ordered to trident forge, which outputs samples not in the order of their original packages, but in the order that you, well, ordered them, if I may go back to my restaurant analogy from an earlier blog post.\nSo with --ordered the output of the above toy example which asked for French, Finnish and Spanish (in that order) will be:\n- Francois (French)\n- Michele (French)\n- Mika (Finnish)\n- Sanni (Finnish)\n- Pedro (Spanish)\nSo I think this will be a very useful feature for creating custom collections."
  },
  {
    "objectID": "posts/xerxes_10.html",
    "href": "posts/xerxes_10.html",
    "title": "Xerxes’s cutlery - enjoying your dinner?",
    "section": "",
    "text": "Image of cutlery, by Unsplash\n\n\nSo, you’ve ordered your food, now what?\nOften, we want to summarise the genetic data through summary statistics. Examples for such summary statistics are F-Statistics. Our tool xerxes fstats, which is part of the Poseidon ecosystem, provides a powerful and expressive tool to compute such summary statistics from genetic data described in Poseidon packages.\nSpecifically, you can for example reproduce the famous F4 statistics that first showed that Neandertalers, a group of close human relatives that disappeared around 45,000 years ago, interbred with modern human ancestors.\nThis is its form: F4(Chimpanzee, Neandertaler, African, European). It is defined in terms of average allele frequency correlation across a set of genetic positions.\nSo, which Chimpanzee, Neandertaler, African and European groups do we have in our datasets on our servers? Let’s find out:\n&gt; trident list --remote --groups | grep Chimp\ntrident v1.3.0.4 for poseidon v2.5.0, v2.6.0, v2.7.0, v2.7.1\nhttps://poseidon-framework.github.io\n\n[Info]    Downloading group data from server\n[Info]    Message from the Server: Greetings from the Poseidon Server, version 1.3.0.4\n| Chimp.REF                                                    | Reference_Genomes                       | 2.2.0           | 1              |\nCool, so there is a group called Chimp.REF in a package called “Reference Genomes” which looks right. What about Neandertals?\n&gt; trident list --remote --groups | grep Neanderthal\ntrident v1.3.0.4 for poseidon v2.5.0, v2.6.0, v2.7.0, v2.7.1\nhttps://poseidon-framework.github.io\n\n[Info]    Downloading group data from server\n[Info]    Message from the Server: Greetings from the Poseidon Server, version 1.3.0.4\n| Altai_Neanderthal.DG                                         | Archaic_Humans                          | 2.2.0           | 1              |\n| Altai_Neanderthal_published.DG                               | Archaic_Humans                          | 2.2.0           | 1              |\n| Chagyrskaya_Neanderthal.SG                                   | 2020_Mafessoni_Neanderthal              | 0.2.0           | 1              |\n| DenisovaNeanderthalMix.SG                                    | Archaic_Humans                          | 2.2.0           | 1              |\n| Goyet_Neanderthal.SG                                         | Archaic_Humans                          | 2.2.0           | 1              |\n| LesCottes_Neanderthal.SG                                     | Archaic_Humans                          | 2.2.0           | 1              |\n| Mezmaiskaya1_Neanderthal.SG                                  | Archaic_Humans                          | 2.2.0           | 1              |\n| Mezmaiskaya2_Neanderthal.SG                                  | Archaic_Humans                          | 2.2.0           | 1              |\n| Spy_Neanderthal.SG                                           | Archaic_Humans                          | 2.2.0           | 1              |\n| VindijaG1_Neanderthal.SG                                     | Archaic_Humans                          | 2.2.0           | 1              |\nCool, those are a lot. We’ll just go with Altai_Neanderthal.DG.\nFor the African and Europeans, you can do it similarly, but I’m just telling you that we want “Yoruba” (from Nigeria) and “French” (from, well, France).\nSo how do you get the data? Easy enough. You can just order your meal, remember? Here is the way to download the packages without even knowing which packages:\ntrident fetch -d ~/poseidon_repo -f 'Chimp.REF,Altai_Neanderthal.DG,Yoruba,French'\nThis will download the relevant packages into a repository location specified by -d. You can then just run:\nxerxes fstats -d ~/poseidon_repo --stat 'F4(Chimp.REF,Altai_Neanderthal.DG,Yoruba,French)'\nHere, -d gives the location of your package repository. This runs and outputs a bunch of stuff, and then closes with:\n.-----------.-----------.----------------------.--------.--------.---------.----------------.--------------------.------------------.-------------------.\n| Statistic |     a     |          b           |   c    |   d    | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife |\n:===========:===========:======================:========:========:=========:================:====================:==================:===================:\n| F4        | Chimp.REF | Altai_Neanderthal.DG | Yoruba | French | 549812  | 1.5083e-3      | 1.5083e-3          | 1.9721e-4        | 7.648265856488313 |\n'-----------'-----------'----------------------'--------'--------'---------'----------------'--------------------'------------------'-------------------'\nThe very last number shows that this statistic is significantly positive (i.e. has a Z-score above 3), which shows that Altai_Neanderthal.DG is significantly more closely related to French than to Yoruba.\nOf course, xerxes fstats can do a lot more, and you can read about it in our documentation.\nWe have also added a detailed whitepaper now, which describes the mathematical details of the methods implemented here. For example, here is the table of listed statistics from the whitepaper:\n\n\n\nA table of F-Statistics\n\n\nF-Statistics are one important analysis tool used in the field, but there are of course a lot more, and we hope to implement some more in xerxes in the future."
  },
  {
    "objectID": "posts/trident_14.html",
    "href": "posts/trident_14.html",
    "title": "Trident’s restaurant - your order please?",
    "section": "",
    "text": "Image of a restaurant, by Unsplash\n\n\nToday we’re releasing a new major release for trident, our command-line tool and workhorse for working with Poseidon packages.\nIf you haven’t checked it out yet, take a look at our documentation. With trident, you can download packages from our web-server, initialise new packages, perform validations, list summaries of packages, and - most importantly - forge new packages out of existing ones. This functionality, forge is more than a merge, and more than a subset tool. It is both. Perhaps a working analogy for what it does is a restaurant. At a restaurant, you order a specific meal, but you don’t care how the ingredients have been packaged, and with which other ingredients they are stored. You just want the chefs to assmble the meal, no matter whether the beans are stored together with the peas or not.\nWith forge, you just show the program where to find the storage room (that’s what we call a base-directory that contains packages), and you tell it which individuals or groups you would like to include or exclude. Forge will then automatically look through all packages and assemble exactly the set of samples that you need in your new package.\nForge comes with a kind of mini-language to specify the ingredients that you need. This mini-language is something that we have clarified and subtly improved in this new release. There are four ways to ask forge to include entities in forge:\n\nInclude an individual by name: You can write &lt;EAS006&gt; (in angular brackets) to ask forge to look for the individual named “EAS006” (I’m glad you ask, it’s a male individual from Southern England who did 1500 years ago and was buried near a town called Eastry today.). If there are multiple versions of this individual, because there are multiple versions of packages, forge looks for this individual only on the latest version.\nInclude an individual by name, group and package: Sometimes, there are several individuals with that same name in your package collection, perhaps because you have multiple versions of the same package, or because for some reason you have already previously forged packages all in the same collection. In such a case, there is a more specific way to define an individual, via the syntax &lt;2022_Gretzinger_AngloSaxons-1.0.0:England_EMA:EAS006&gt;, which now specifies a very specific individual that is found in the given package and in the given group.\nInclude entire groups: You can write England_EMA to ask forge to look for all individuals that are associated with the group named “England_EMA” (I’m again glad you ask: EMA stands for “Early Middle Ages”). This again implicitly looks for the group only in the latest versions of packages.\nInclude entire packages: You can write *2022_Gretzinger_AngloSaxons* to ask forge to select for all individuals in the entire package.\n\nNote that you can combine these requests to merge them into the final forge. So for example you could say &lt;EAS006&gt;,&lt;EAS001&gt;,England_IronAge or so, which would then result in a package consisting of the two specified individuls plus all individuals from group “England_IronAge”. So this provides you with a lot of power already to select large numbers of samples in a succinct way. But sometimes you need even more power, because you want to also specifically exclude some samples. In forge, you can do that by prepending a minus-sign to any entity:\n\nExclude an individual by name: Write -&lt;EAS006&gt; to ask forge to remove any sample with that name from the final selection, in case it has been selected before.\nExclude an individual by name, group and package: Write -&lt;2022_Gretzinger_AngloSaxons-0.1.0:England_EMA:EAS006&gt; to not remove all individual with the name “EAS006”, but only the one in the specified package.\nExclude an entire group: Write -England_EMA to remove all individuals that belong to that group.\nExclude an entire package: Write -*2022_Gretzinger_AngloSaxons* to remove this package.\n\nSo now you can accurately describe what you want to include and exclude. Note that the order matters, as you would expect: *2022_Gretzinger_AngloSaxons*,-England_EMA,&lt;EAS006&gt; will contain a selection that includes &lt;EAS006&gt;, since it was added back at the end. In contrast, *2022_Gretzinger_AngloSaxons*,&lt;EAS006&gt;,-England_EMA will not include the sample, as the entire group was removed after it was added, so it got removed again.\nThere is one more gist: If your forge-list starts with an exclusion statement, so if you just write -*2022_Gretzinger_AngloSaxons*, then this implicitly means that you include all latest versions of packages in the base directory. So this is a way to forge everything except a given package or group or individual.\nThe exact semantics of this forge-machinery are quite complex and powerful, and we have clarified how this works in the latest version, and made it specifically more consistent and intuitive. This in particular concerns a feature that we now built in for the first time: You can now append versions of packages explicitly to a package-name, to select a specific version of a package. This works both in package requests, and in individuals by name, group and package.\nSo *2022_Gretzinger_AngloSaxons-1.2.1* works, and &lt;2022_Gretzinger_AngloSaxons-1.2.1:England_EMA:EAS006&gt; works as well.\nThere was quite a bunch of refactoring of code that needed to happen under the hood to make this update possible, in particular in relation to supporting multiple versions of packages. This update will bring us a good step closer to making archaeoenetics analyses truly reproducible, as older versions of packages will continue to be served on our servers (which we will post about separately)."
  },
  {
    "objectID": "posts/isba2023poster.html",
    "href": "posts/isba2023poster.html",
    "title": "The Poseidon poster at ISBA 2023",
    "section": "",
    "text": "The biennial meeting of the International Society for Biomolecular Archaeology (ISBA) is one of the key conferences for human archaeogenetic research. This years meeting, ISBA2023 was set to take place from 13th to 16th September in the city of Tartu in Estonia and entitled “New Horizons in Biomolecular Archaeology”. Naturally we decided to present Poseidon there to its core user group: practitioners in the field of archaeogenetics.\nWe applied with an ambitious abstract (the full text is available at the bottom of this post) and were awarded a poster. Below we share what we came up with. We tried to explain what motivates us to work on Poseidon and which components of the framework are already available. This includes the package format specification, the software tools, the public archives and the Minotaur workflow.\nThiseas C. Lamnidis and I finally presented the poster as the result of collaborative work with Ayshin Ghalichi, Dhananjaya B. A. Mudiyanselage, Wolfgang Haak and Stephan Schiffels in Tartu.\nYou can download the complete poster in .pdf format here."
  },
  {
    "objectID": "posts/isba2023poster.html#abstract",
    "href": "posts/isba2023poster.html#abstract",
    "title": "The Poseidon poster at ISBA 2023",
    "section": "Abstract",
    "text": "Abstract\nPoseidon – Powerful and FAIR archaeogenetic genotype data management\nThe increase in ancient human DNA data requires robust yet flexible solutions to store and distribute it. While raw sequencing data is generally shared in large archives (e.g. ENA or SRA), no such standard exists for derived genotype data and contextual, spatiotemporal information. This raises concrete issues:\n\nAncient individuals only constitute meaningful observations if their spatiotemporal provenience is known. Current practice renders it difficult to maintain the connection between archaeological context and sampled genomic data.\nSpecific results can only be reproduced with genotypes. Current practice does not include their publication.\nMeta-analyses involving large amounts of data require tedious curation. Available public datasets have high key-person-risk, lag behind and are not well machine-readable.\n\nTo tackle these issues we present Poseidon, a computational framework including an open data format, software, and community-maintained archives to enable standardised and FAIR handling of archaeogenetic data. A Poseidon package bundles genotypes in industry-standard formats with human- and machine-readable context data, storing sample-wise information on spatiotemporal origin and genetic data quality. Through collective effort we already prepared 100+ packages for published studies in an open online repository.\nIn our talk we want to specifically highlight two components of the framework:\n\nA computational platform to process raw sequencing data directly from archive data (e.g. ENA), with a public interface for package curators to control processing parameters. Infrastructure will be provided by MPI-EVA, with community access and continuous integration provided via GitHub.\nCommand line software for package validation, manipulation and analysis. Validating structural integrity is a core task to honour the FAIR promise. The reliable structure then allows for advanced stream processing and co-analysis of genomic and contextual information."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Poseidon Blog",
    "section": "",
    "text": "Welcome to the Poseidon-Blog.\nIf you want to get back to the Poseidon Website, click here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoseidon archive management: editing a pull request branch created from a fork\n\n\n \n\n\ntechnotes\n\nworkflows\n\n\n \n\nA quick, technical write-up of a Git editing workflow particularly useful for the Poseidon community archive.\n\n\n\nFeb 21, 2025\nClemens Schmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrident update: Ordered forge\n\n\n \n\n\nfeatures\n\n\n \n\nMore control -- Ordered forges\n\n\n\nFeb 11, 2025\nStephan Schiffels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023 ♥ Poseidon\n\n\n \n\n\nreviews\n\n\n \n\nAn opinionated summary of the year's events in and around the Poseidon framework. I look back on 2023's developments in the Poseidon schema, the Minotaur workflow, the public archives and the Poseidon software tools.\n\n\n\nDec 31, 2023\nClemens Schmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Ancient DNA Landscape: Introducing the Poseidon Archive Explorer\n\n\n \n\n\n\n\n\nNov 29, 2023\nDhananjaya Aththanayaka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXerxes's cutlery - enjoying your dinner?\n\n\n \n\n\nnews\n\n\n \n\nA new update of xerxes, more statistics and documentation\n\n\n\nOct 27, 2023\nStephan Schiffels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrident's restaurant - your order please?\n\n\n \n\n\nnews\n\n\n \n\nA new update of trident, improved semantics of forge queries\n\n\n\nOct 4, 2023\nStephan Schiffels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Poseidon poster at ISBA 2023\n\n\n \n\n\nnews\n\nconferences\n\n\n \n\nHere is the poster we presented at the ISBA conference in Tartu, Estonia.\n\n\n\nSep 24, 2023\nClemens Schmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the Poseidon-Blog\n\n\n \n\n\nnews\n\n\n \n\nA new update of xerxes, more statistics and documentation\n\n\n\nSep 22, 2023\nStephan Schiffels\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/endofyear2023.html",
    "href": "posts/endofyear2023.html",
    "title": "2023 ♥ Poseidon",
    "section": "",
    "text": "It’s late December and the time of the year when work slows down in my part of the world. For many of us an opportunity to take a break and to look back, contemplating the achievements of the year. I decided to do so as well and write a bit about Poseidon.\nWhat follows is a subjective account of the events in and around the framework in 2023 - each of my colleagues in the core team (Stephan Schiffels, Ayshin Ghalichi, Thiseas C. Lamnidis, Dhananjaya B. A. Mudiyanselage, Wolfgang Haak and I, Clemens Schmid) would probably emphasise different developments in such a write-up. That is in itself an achievement, because it shows how much the tech-stack, domains and services in our little ecosystem have grown this year: beyond the understanding of each of us individually."
  },
  {
    "objectID": "posts/endofyear2023.html#the-poseidon-schema",
    "href": "posts/endofyear2023.html#the-poseidon-schema",
    "title": "2023 ♥ Poseidon",
    "section": "The Poseidon schema",
    "text": "The Poseidon schema\nLet’s start simple with the two new releases of the Poseidon schema we published this year: v2.7.0 and v2.7.1. They were published in short succession in March and May, the latter only slightly improving the sequencing source files (.ssf) added in the first. See the changelog here for more details, but the addition of the .ssf file is indeed their most remarkable contribution to the schema. With it we addressed a major desideratum and unresolved question in previous versions of Poseidon: How should genotype data be linked to the raw sequencing data on the European Nucleotide Archive (ENA) and other archives of the International Nucleotide Sequence Database Collaboration (INSDC)?\nThe .ssf file is, I would argue, a smart solution for this question. It specifies the same variables already used in the ENA database, allows for an extremely flexible, yet not arbitrary n:m connection between the entities in a Poseidon package and the raw data products and it can be generated semi-automatically for most of the data in our public archives. With some tweaking it can also be used to organize local data repositories independent of any online databases. The .ssf file is finally the very foundation on top of which the amazing Minotaur workflow is built (see below).\nGenerally, both the fact that only two Poseidon releases were necessary this year and that we could treat them as non-breaking changes indicate that we reached a certain level of maturity and stability in the schema. Of course we still have ideas how to extend it further in the future, but at the moment I’m optimistic that we can maintain long-term backwards compatibility. The process in which we discussed, specified and later improved the .ssf file definition to then see Minotaur be erected on top of it was a very satisfying professional experience for me personally."
  },
  {
    "objectID": "posts/endofyear2023.html#the-minotaur-workflow",
    "href": "posts/endofyear2023.html#the-minotaur-workflow",
    "title": "2023 ♥ Poseidon",
    "section": "The Minotaur workflow",
    "text": "The Minotaur workflow\nThe Minotaur workflow is a semi-automatic workflow to reproducibly process published sequencing data into Poseidon packages. Developing this entirely new branch of the Poseidon ecosystem became possible because Thiseas joined the Poseidon core team in 2023. He came up with a sophisticated, yet open and transparent implementation of this process, in which authors and the community as a whole retain control over the data and the data processing parameters. A full write-up for the website is in progress. Here is the summary Thiseas prepared for our poster at the ISBA conference:\nCommunity members can request new packages to be processed through the Minotaur workflow by submitting a build recipe as a pull request against a dedicated GitHub repository. This recipe is created from a sequencing source file (.ssf), describing the sequencing data for the package and where it can be downloaded. Using the recipe, the sequencing data gets processed via nf-core/eager on computational infrastructure of MPI-EVA, using a standardised, yet flexible, set of parameters. The generated genotypes, together with descriptive statistics of the sequencing data (Endogenous, Damage, Nr_SNPs, Contamination), are compiled into a Poseidon package, and made available to users in the minotaur-archive.\nThe Minotaur workflow is a timely addition to the Poseidon framework, providing a flexible solution to wrap legacy and new data in uniformly processed packages. Homogeneous data processing puts us closer to our great comparadum, the AADR dataset. It also helped us to finalize the structure of our public archives, which emerged from long discussions about the kind of data we think the aDNA community requires for derived analyses.\nRight now the Minotaur workflow is still in a final development and testing phase, where we focus on the processes around it, so the submission of recipes, their review and the forwarding of results to the minotaur-archive. One particular tricky question is how context information in the .janno file should be passed from the community-archive to the new packages in the minotaur-archive. One of the last pull requests for our software tool trident in 2023 aims to introduce a reliable mechanism to merge .janno files to address this issue."
  },
  {
    "objectID": "posts/endofyear2023.html#the-public-archives",
    "href": "posts/endofyear2023.html#the-public-archives",
    "title": "2023 ♥ Poseidon",
    "section": "The public archives",
    "text": "The public archives\nIn 2023 we finally came to a conclusion on how to organize our public data archives. What emerged is a threefold division into what we call the community-archive, the minotaur-archive and the aadr-archive. The archives are described in more detail on the website, but here’s the gist of it:\nThe community-archive emerged from our old public-archive. It includes the legacy data we originally copied from the AADR. We now decided to use this archive for author-submitted publication-wise packages to collect the exact genotype data analysed in the respective papers. The idea is twofold: With the author-submitted genotype data the results in a given paper can be reproduced exactly. And the publication authors are generally the most trustworthy authority for the context data we collect in the .janno files, e.g. the spatiotemporal origin of the individual samples. Ayshin and I recently wrote about the submission process for the community-archive here.\nThe minotaur-archive mirrors the community-archive in that it features publication-wise packages, usually even the very same as in the community-archive. To distinguish them clearly, package titles and sample-wise Poseidon_IDs in the minotaur-archive carry the suffix _MNT. As explained above the packages in this archive include consistently reprocessed genotype data, run through the Minotaur workflow.\nThe aadr-archive is the conceptionally most simple archive. It features “poseidonized” versions of releases of the AADR dataset, currently only the latest AADR v54.1.p1. We documented the code and decisions for the cleaning and packaging process here.\n2023 not only saw the planning and setup of these three archives, but also a lot of work to fill them with life. For the community archive that meant plenty of data cleaning by all of us, most notably Dhananjaya. And it also meant providing guidance for authors to submit their data. Thanks to the hard work of Ayshin a total of eleven author-submitted packages are available in the archive now. Number twelve was submitted shortly before christmas and is awaiting review. The minotaur-archive is still functionally empty, but three packages are pending thanks to Thiseas and will hopefully soon be merged. Preparing the latest version of the AADR dataset for the aadr-archive was one of the projects I tackled this year."
  },
  {
    "objectID": "posts/endofyear2023.html#the-software-tools",
    "href": "posts/endofyear2023.html#the-software-tools",
    "title": "2023 ♥ Poseidon",
    "section": "The software tools",
    "text": "The software tools\nThe Poseidon software tools grew significantly more powerful this year. From a user-perspective 2023 brought various new features, changes to the command line interfaces and breaking updates in the Web-API. To keep track of the releases and the Poseidon schema versions they support I created a version overview table on the website.\nWith qjanno I added an entirely new tool to the set. It is a command line tool to run SQL queries on .janno (and arbitrary .csv and .tsv) files. I created it by forking the qsh package and then adjusting it heavily for the use on Poseidon packages. Just as trident it is written in Haskell and openly available with precompiled executables here.\nStephan invested a good amount of effort into consolidating the data analysis features in xerxes. He wrote a whitepaper to explain and justify the reasoning behind the implemented logic for f-statistics, and another blog post on how to run it. Even more approachable and comprehensive is a write-up he shared here. Together we worked on integrating the many changes to trident and its underlying poseidon-hs Haskell library into xerxes.\nOur main workhorse, trident, saw an astonishing number of new releases: v1.1.6.0 on January 8 to v1.4.0.3 on October 30. I quickly went through the extended changelogs published with each release to summarize the user-facing highlights of what trident supports now:\n\nArbitrary columns in the .janno file beyond the columns specified in the Poseidon schema (v1.1.6.0)\nSpecification of individuals with identical names from different source packages in the trident forge selection language (v1.1.7.0)\nValidation of the entire genotype data in a package with --fullGeno in trident validate (v1.1.10.2)\nPoseidon schema version v2.7.1 with validation of the .ssf file (v1.1.12.0)\nA highly improved Poseidon Web-API that allows to request individual (old) package versions (v1.2.0.0)\nReworked versions of trident update, now called trident rectify, and trident validate, which now allows to validate not just entire packages, but also individual files (v1.3.0.4)\nSelecting packages by version in the forge selection language and generally handling multiple package versions (v1.4.0.2, Stephan shared yet another blog post about this release)\n\nAs always I enjoyed the work on the software tools tremendously, especially in two cases: If one of our users reports an issue and we can address a concrete need with a release, and if the Haskell programming language allows for a particularly elegant solution for a given problem. A currently pending pull request combines both: Ayshin made me aware of some validation failure cases that require better error messages and I found a neat way to provide just that with a custom-tailored monadic stack."
  },
  {
    "objectID": "posts/endofyear2023.html#outreach",
    "href": "posts/endofyear2023.html#outreach",
    "title": "2023 ♥ Poseidon",
    "section": "Outreach",
    "text": "Outreach\nThe last domain where we made good progress in 2023 is public outreach. Naturally we invested hours in writing and updating documentation on the project website (https://www.poseidon-adna.org), but we also pursued a number of special projects beyond the basic, technical description of software and workflows.\nThe first one of these was possible thanks to the effort of Dhananjaya, Stephan and me: We built a page on the website where the data in the public archives can be easily explored. It makes use of our Web-API to access the data and display it with a sub-page for each package. Dhananjaya wrote a blog post about this, recently.\nI already mentioned this blog multiple times above. It is indeed another great addition of 2023. Stephan created a separate website at https://blog.poseidon-adna.org to share news and short tutorials. Our wish has always been to gather an active and engaged community of users around Poseidon, and we hope to establish this blog as one of its central communication hubs. A major medium for longer write-ups beyond the technical documentation already available on the website.\nTo announce our blog posts, software releases and other news we fully switched from Twitter (now X) to the Fediverse in 2023. You can follow us here: https://ecoevo.social/@poseidon. The switch came naturally, given the state of affairs at X. Submitting posts automatically is more easy with Mastodon compared to Twitter and I made sure that this process works reliably for our software releases on GitHub.\nBeyond these technical novelties and online communication we also presented Poseidon at two in-person conferences in 2023: ISBA10 in Tartu, Estonia and the NFDI4Objects community meeting in Berlin, Germany. The poster we presented at both of these occasions was already mentioned above and is available here. And the slides for the talk Thiseas prepared for the latter should soon be made available by the NFDI4Objects team."
  },
  {
    "objectID": "posts/endofyear2023.html#conclusion",
    "href": "posts/endofyear2023.html#conclusion",
    "title": "2023 ♥ Poseidon",
    "section": "Conclusion",
    "text": "Conclusion\nMuch has happened for Poseidon in 2023 and I’m sure I’m not doing all of it due justice in this little summary. But I consider what is here already an impressive list that stands witness for the effort we put into the framework. And it seems to pay off: The user base is growing. More users help us in turn to find and address remaining issues and make Poseidon better for all of us. This will once more be one of my main aspirations in the coming year 2024."
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Welcome to the Poseidon-Blog",
    "section": "",
    "text": "Welcome to the Poseidon Blog. This is a place where the core developer team posts news, tutorials, or insights about the Poseidon Ecosystem. We welcome feedback to our posts, which you can give via Mastodon and our Slack Channel.\nPoseidon has been a project developed internally at the Max Planck Institute for Evolutionary Anthropology, and is partially funded by the ERC-project MICROSCOPE.\nOur core mission is to make the growing field of archaeogenetics more [FAIR] and open, and to make data analysis of archaeogenetic data more widely and multidisciplinarily applicable. To this end, we intend to provide teaching material, make data available in standardised formats, and provide well-documented tools. Stay tuned for more development, and follow us on Mastodon and Slack."
  },
  {
    "objectID": "posts/Archive_explorer_Blogpost.html",
    "href": "posts/Archive_explorer_Blogpost.html",
    "title": "Navigating the Ancient DNA Landscape: Introducing the Poseidon Archive Explorer",
    "section": "",
    "text": "In the ever-evolving world of ancient DNA research, staying up-to-date with the latest publications and datasets is essential. To address the challenges faced by researchers in this field, the Poseidon ecosystem has introduced a new feature - the Poseidon Archive Explorer. This dynamic web application offers a comprehensive overview of current Ancient DNA publications, providing a valuable resource for scholars and enthusiasts alike.\n\n\n\narchive explorer main UI\n\n\nIn the Poseidon ecosystem, there are three main components:\n\nThe Poseidon Package Format: This component serves as a standardized framework for organizing genotype data, along with associated meta- and contextual information. It offers a structured, yet adaptable, format that is easily readable by both humans and machines.\nThe Poseidon Software: This component encompasses a suite of in-house developed software tools and the Poseidon R package. These resources are designed to facilitate the efficient management of genotype data, ensuring ease and precision in data handling.\nPublic repositories : Poseidon currently maintains 3 Public repositories. The Poseidon Community Archive, The Poseidon Minotaur Archive, and The Poseidon AADR Archive. These repositories are carefully organized to store Ancient DNA genotyping data together with archaeological and laboratory context information. To learn more about each of their specific roles, you can visit this link\n\nThe Poseidon Ecosystem further underscores its commitment to data integrity by integrating an in-house web API with the GitHub Large File Storage system. This hybrid approach ensures robust version control and data cleanliness. Researchers and enthusiasts can benefit from this combination, fostering a harmonious ecosystem of data accessibility and precision.\n\n\n\nUI demo\n\n\nA Bridge Between Data and You\nThe concept behind the development of Archive Explorer is pretty straightforward. We’ve been housing all our Poseidon genotype data using the GIT Large File Storage system (GIT LFS). While that’s all fine and dandy for version control, we realized we needed something more interactive and user-friendly to share and visualize this data with the public. The solution? A dynamic web application built on JavaScript. When you land on the Archive Explorer page, you’ll notice it’s got some neat features. Up top, there’s an archive selector. This is your gateway to choose the specific archive you’re interested in. Then, we’ve thrown in a world map powered by the Leaflet map plugin because it’s open source and super easy to use.\nThis map? It’s not your regular map. It’s more like a bird’s-eye view of all the genotype data scattered across the globe. You can see data points from around the world, making it easy to grasp the big picture.\nThree Paths to Poseidon Genotype Data: Take Your Pick\nWe give you three different routes to get to the Poseidon genotype data you’re after:\n\nIf you know exactly what you’re looking for, you can type a full or partial name in our dynamic search bar and select the package you want.\nWant to explore data from a specific part of the world? Check out the world map, click on a marker, and voila! You’ll see a snapshot of what’s in that package, and a link to dive straight into it.\nFor those who prefer the manual approach, we’ve got a table view of all the publications available in each archive. You can roll up your sleeves and search for the info you need.\n\n\n\n\npackage view\n\n\n\n\n\npackage view\n\n\nGetting Up Close and Personal with the Data\nOnce you’ve picked a package, you can access it either through the map marker or by hitting that trusty magnifying glass icon on the table view. If you’re into details, the table view lets you view genotype data or download it as a “zip” file via our inhouse Poseidon server.\nExploring the Package: More Than Meets the Eye\nThe package view isn’t all that different from the general user interface, but it’s filtered to show only the markers that are relevant to that particular publication. And if you want to dig deeper, just click on each “PoseidonID” to access more detailed genotype information. We’re also cooking up a new feature to display some informative descriptive statistics for each publication, giving you a broader picture of the data. So stay tuned!\nIn a nutshell, the Archive Explorer is your one-stop shop for navigating the world of Poseidon genotype data. It’s got the goods for the pros and is user-friendly enough for the rest of us to dive into the fascinating realm of ancient DNA research."
  },
  {
    "objectID": "posts/archive_editing_git.html",
    "href": "posts/archive_editing_git.html",
    "title": "Poseidon archive management: editing a pull request branch created from a fork",
    "section": "",
    "text": "At the time of writing, the Poseidon community archive has 14 open pull requests – most of which were opened by various community members to add new packages to the archive. What certainly is a pleasant development, because it indicates that the archive gets adopted, also comes with technical and administrative challenges. As an editor for the archive I recently had to step up my Git skills to address a particular issue I was facing.\nAlready multiple times I found myself in the situation that I needed to edit a submission pull request before merging. This arose for example, when a package author prepared a package almost perfectly, but I still wanted to apply some additional minor changes before merging. Or an author or reviewer had struggled with Git, manoeuvred themselves into a predicament, and needed my help to untangle the knot without starting from scratch. So here is what I came up with to allow me to do that efficiently."
  },
  {
    "objectID": "posts/archive_editing_git.html#workflow",
    "href": "posts/archive_editing_git.html#workflow",
    "title": "Poseidon archive management: editing a pull request branch created from a fork",
    "section": "Workflow",
    "text": "Workflow\nGitHub’s documentation includes a helpful tutorial how to commit changes to a pull request branch created from a fork. It already covers the basic workflow how to edit a fork. The article highlights a number of conditions for this to be possible:\n\nYou can only make commits on pull request branches that:\n\nAre opened in a repository that you have push access to and that were created from a fork of that repository\nAre on a user-owned fork\nHave permission granted from the pull request creator\nDon’t have branch restrictions that will prevent you from committing\n\n\nAll of these are met in my case. But two additional challenges complicate the matter: i) the community-archive uses Git LFS for the large data files, and ii) I need to do this so frequently, that cloning every fork feels unnecessarily cumbersome. The following workflow considers this special situation.\n\n1. Clone the fork repository\nGIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/USERNAME/FORK-OF-THE-REPOSITORY\nNote that this workflow assumes that you have installed and configured Git LFS on your system. Cloning the repo with the GIT_LFS_SKIP_SMUDGE environment variable prevents downloading the LFS-tracked files despite Git LFS being enabled. This saves bandwidth and costs for us on GitHub.\n\n\n2. (if necessary) Switch to the pull request branch\ngit switch PR-BRANCH\nThis is only necessary, if the PR branch is not the main/master branch.\n\n\n3. (if necessary) Download individual LFS-tracked files\ngit lfs pull --include \"PATH-TO-FILE\"\nTo validate a package completely it can be necessary to also access the genotype data. But because we cloned above with GIT_LFS_SKIP_SMUDGE=1, this data is not in our clone now. Fortunately we can selectively download it. PATH-TO-FILE can also include wildcards.\n\n\n4. Edit the files as desired\nRemember to commit the changes.\n\n\n5. Push the changes\nThis should work with git push. But yet again, Git LFS complicates things, raising the following error message:\nerror: Authentication error: Authentication required: You must have push access to verify locks\nerror: failed to push some refs to 'github.com:USERNAME/FORK-OF-THE-REPOSITORY.git'\nThis is caused by a limitation of GitHub’s Git LFS implementation. A long thread here discusses the issue: Authentication required : You must have push access to verify locks error. Multiple solutions are suggested there. One reliable workaround is to delete the git hook .git/hooks/pre-push.\nrm .git/hooks/pre-push\ngit push\nThis resolved the issue for me – specifically because I never had to edit any of the genotype data files when working on a PR fork. I don’t know how this hack affects the handling of LFS-tracked files.\n\n\n(optional) 6. Moving to another fork in the same clone\nIf the changes in a fork A are already merged into the master branch of the main archive repository, then a little trick allows to switch to another fork B in the same clone.\ngit remote -v\ngit remote set-url origin git@github.com:poseidon-framework/community-archive.git\ngit switch master\ngit pull\ngit remote set-url origin git@github.com:USERNAME/FORK-OF-THE-NEXT-REPOSITORY.git\ngit pull\nWe set the remote URL to the main repository, switch to the master branch, and pull. The commits from A are already there, so we have a clean state again. From here we can set a new remote URL for a fork B and pull. This effectively saves us from creating a fresh clone (as described in Section 1.1)."
  }
]